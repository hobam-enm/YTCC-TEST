# ğŸ“Š ìœ íŠœë¸Œ ë°˜ì‘ ë¦¬í¬íŠ¸: AI ëŒ“ê¸€ìš”ì•½ (Streamlit Cloudìš© / ë™ì‹œì‹¤í–‰ 1 ìŠ¬ë¡¯ ë½ í¬í•¨)

import streamlit as st
import pandas as pd
import io, os, json, re, time, shutil
from datetime import datetime, timedelta, timezone
from urllib.parse import urlparse, parse_qs
from concurrent.futures import ThreadPoolExecutor, as_completed

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaFileUpload
from google.oauth2 import service_account

import google.generativeai as genai

import plotly.express as px
from plotly import graph_objects as go
import circlify
import stopwordsiso as stopwords
from collections import Counter
from kiwipiepy import Kiwi
import numpy as np

try:
    from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE
except Exception:
    ILLEGAL_CHARACTERS_RE = None

# ===================== ê¸°ë³¸ ê²½ë¡œ(Cloud) =====================
BASE_DIR = "/tmp"  # Streamlit CloudëŠ” /tmpë§Œ ì“°ê¸° ê°€ëŠ¥(íœ˜ë°œì„±)
SESS_DIR = os.path.join(BASE_DIR, "sessions")
os.makedirs(SESS_DIR, exist_ok=True)

# ===================== ë¹„ë°€í‚¤ / íŒŒë¼ë¯¸í„° =====================
# secrets ìš°ì„ , ì—†ìœ¼ë©´ í•˜ë“œì½”ë”© ë°±ì—…
_YT_FALLBACK = []
_GEM_FALLBACK = []

YT_API_KEYS = list(st.secrets.get("YT_API_KEYS", [])) or _YT_FALLBACK
GEMINI_API_KEYS = list(st.secrets.get("GEMINI_API_KEYS", [])) or _GEM_FALLBACK
GEMINI_MODEL = "gemini-2.0-flash-lite"
GEMINI_TIMEOUT = 120
GEMINI_MAX_TOKENS = 2048

# Google Drive ì„¤ì •
GDRIVE_PARENT_FOLDER_ID = st.secrets.get("GDRIVE_PARENT_FOLDER_ID", "")
_GDRIVE_KEYS_RAW = [st.secrets.get("GDRIVE_KEY_1"), st.secrets.get("GDRIVE_KEY_2"), st.secrets.get("GDRIVE_KEY_3")]
GDRIVE_KEYS = []
for raw in _GDRIVE_KEYS_RAW:
    if raw:
        try:
            GDRIVE_KEYS.append(json.loads(raw))
        except Exception:
            try:
                GDRIVE_KEYS.append(json.loads(str(raw)))
            except Exception:
                pass

# ìˆ˜ì§‘ ìƒí•œ(í•„ìš”ì‹œ ì¡°ì •)
MAX_TOTAL_COMMENTS = 200_000
MAX_COMMENTS_PER_VIDEO = 5_000

# ===================== ë™ì‹œ ì‹¤í–‰ 1 ìŠ¬ë¡¯(ë½ íŒŒì¼) =====================
LOCK_PATH = os.path.join(BASE_DIR, "ytccai.busy.lock")

def try_acquire_lock(ttl=7200):
    # ì˜¤ë˜ëœ ë½ ì •ë¦¬
    if os.path.exists(LOCK_PATH):
        try:
            if time.time() - os.path.getmtime(LOCK_PATH) > ttl:
                os.remove(LOCK_PATH)
        except:
            pass
    if os.path.exists(LOCK_PATH):
        return False
    open(LOCK_PATH, "w").close()
    return True

def refresh_lock():
    try: os.utime(LOCK_PATH, None)
    except: pass

def release_lock():
    try:
        if os.path.exists(LOCK_PATH):
            os.remove(LOCK_PATH)
    except:
        pass

def lock_guard_start_or_warn():
    """ê¸´ ì‘ì—… ì‹œì‘ ì „ì— í˜¸ì¶œ: ë½ì„ ì¡ê³  True ë°˜í™˜, ì‹¤íŒ¨ì‹œ ê²½ê³  í›„ False"""
    if not try_acquire_lock():
        st.warning("ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ì‘ì—… ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return False
    return True

# ===================== ê¸°ë³¸ UI =====================
st.set_page_config(page_title="ğŸ“Š ìœ íŠœë¸Œ ë°˜ì‘ ë¦¬í¬íŠ¸: AI ëŒ“ê¸€ìš”ì•½", layout="wide", initial_sidebar_state="collapsed")
st.title("ğŸ“Š ìœ íŠœë¸Œ ë°˜ì‘ ë¶„ì„: AI ëŒ“ê¸€ìš”ì•½")
st.caption("ë¬¸ì˜ì‚¬í•­:ë¯¸ë””ì–´)ë””ì§€í„¸ë§ˆì¼€íŒ…íŒ€ ë°ì´í„°íŒŒíŠ¸")

_YT_ID_RE = re.compile(r'^[A-Za-z0-9_-]{11}$')
def _kst_tz(): return timezone(timedelta(hours=9))
def kst_to_rfc3339_utc(dt_kst: datetime) -> str:
    if dt_kst.tzinfo is None: dt_kst = dt_kst.replace(tzinfo=_kst_tz())
    return dt_kst.astimezone(timezone.utc).isoformat().replace("+00:00","Z")

def clean_illegal(val):
    if isinstance(val, str) and ILLEGAL_CHARACTERS_RE is not None:
        return ILLEGAL_CHARACTERS_RE.sub('', val)
    return val

# ===================== í˜•íƒœì†Œ/ë¶ˆìš©ì–´ =====================
kiwi = Kiwi()
korean_stopwords = stopwords.stopwords("ko")

# ===================== (ë¡œê·¸ ì œê±°) append_log â†’ no-op =====================
def append_log(*args, **kwargs):
    # ë¡œê·¸ ë¹„í™œì„±í™”: ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ
    return

# ===================== í‚¤ ë¡œí…Œì´í„° (ë²”ìš©í™”) =====================
class RotatingKeys:
    def __init__(self, keys, state_key: str, on_rotate=None, treat_as_strings: bool = True):
        cleaned = []
        for k in (keys or []):
            if k is None:
                continue
            if treat_as_strings and isinstance(k, str):
                ks = k.strip()
                if ks:
                    cleaned.append(ks)
            else:
                cleaned.append(k)
        self.keys = cleaned[:10]
        self.state_key = state_key
        self.on_rotate = on_rotate
        idx = st.session_state.get(state_key, 0)
        self.idx = 0 if not self.keys else (idx % len(self.keys))
        st.session_state[state_key] = self.idx
    def current(self):
        if not self.keys: return None
        return self.keys[self.idx % len(self.keys)]
    def rotate(self):
        if not self.keys: return
        self.idx = (self.idx + 1) % len(self.keys)
        st.session_state[self.state_key] = self.idx
        if callable(self.on_rotate): self.on_rotate(self.idx, self.current())

# ===================== API í˜¸ì¶œ ë˜í¼ =====================
def is_youtube_quota_error(e: HttpError) -> bool:
    try:
        data = json.loads(getattr(e, "content", b"{}").decode("utf-8", errors="ignore"))
        status = getattr(getattr(e, 'resp', None), 'status', None)
        if status in (403, 429):
            reasons = [(err.get("reason") or "").lower() for err in data.get("error", {}).get("errors", [])]
            msg = (data.get("error", {}).get("message", "") or "").lower()
            quota_flags = ("quotaexceeded", "dailylimitexceeded", "ratelimitexceeded")
            if any(r in quota_flags for r in reasons): return True
            if "rate" in msg and "limit" in msg: return True
            if "quota" in msg: return True
        return False
    except Exception:
        return False

def with_retry(fn, tries=2, backoff=1.4):
    for i in range(tries):
        try:
            return fn()
        except HttpError as e:
            status = getattr(getattr(e, 'resp', None), 'status', None)
            if status in (400, 401, 403) and not is_youtube_quota_error(e):
                raise
            if i == tries - 1: raise
            time.sleep((i + 1) * backoff)
        except Exception:
            if i == tries - 1: raise
            time.sleep((i + 1) * backoff)

class RotatingYouTube:
    def __init__(self, keys, state_key="yt_key_idx", log=None):
        self.rot = RotatingKeys(keys, state_key, on_rotate=lambda i, k: log and log(f"ğŸ” YouTube í‚¤ ì „í™˜ â†’ #{i+1}"))
        self.log = log
        self.service = None
        self._build_service()
    def _build_service(self):
        key = self.rot.current()
        if not key:
            raise RuntimeError("YouTube API Keyê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        self.service = build("youtube", "v3", developerKey=key)
    def _rotate_and_rebuild(self):
        self.rot.rotate(); self._build_service()
    def execute(self, request_factory, tries_per_key=2):
        attempts = 0
        max_attempts = len(self.rot.keys) if self.rot.keys else 1
        while attempts < max_attempts:
            try:
                req = request_factory(self.service)
                return with_retry(lambda: req.execute(), tries=tries_per_key, backoff=1.4)
            except HttpError as e:
                if is_youtube_quota_error(e) and len(self.rot.keys) > 1:
                    self._rotate_and_rebuild()
                    attempts += 1
                    continue
                raise

def is_gemini_quota_error(exc: Exception) -> bool:
    msg = (str(exc) or "").lower()
    return ("429" in msg) or ("too many requests" in msg) or ("rate limit" in msg) or ("resource exhausted" in msg) or ("quota" in msg)

def call_gemini_rotating(model_name: str, keys, system_instruction: str, user_payload: str,
                         timeout_s: int = GEMINI_TIMEOUT, max_tokens: int = GEMINI_MAX_TOKENS, on_rotate=None) -> str:
    rot = RotatingKeys(keys, state_key="gem_key_idx", on_rotate=lambda i, k: on_rotate and on_rotate(i, k))
    if not rot.current():
        raise RuntimeError("Gemini API Keyê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    attempts = 0
    max_attempts = len(rot.keys) if rot.keys else 1
    while attempts < max_attempts:
        try:
            genai.configure(api_key=rot.current())
            model = genai.GenerativeModel(
                model_name,
                generation_config={"temperature": 0.2, "max_output_tokens": max_tokens, "top_p": 0.9}
            )
            resp = model.generate_content([system_instruction, user_payload],
                                          request_options={"timeout": timeout_s})
            out = getattr(resp, "text", None)
            if not out and hasattr(resp, "candidates") and resp.candidates:
                c = resp.candidates[0]
                if hasattr(c, "content") and getattr(c.content, "parts", None):
                    part = c.content.parts[0]
                    if hasattr(part, "text"):
                        out = part.text
            return out or ""
        except Exception as e:
            if is_gemini_quota_error(e) and len(rot.keys) > 1:
                rot.rotate(); attempts += 1; continue
            raise

# ===================== Google Drive (ì„œë¹„ìŠ¤ê³„ì • ë¡œí…Œì´ì…˜) =====================
_GOOGLE_SCOPES = ["https://www.googleapis.com/auth/drive"]
def _build_drive_service_from_creds_dict(creds_dict: dict):
    creds = service_account.Credentials.from_service_account_info(creds_dict, scopes=_GOOGLE_SCOPES)
    return build("drive", "v3", credentials=creds, cache_discovery=False)

class RotatingDrive:
    def __init__(self, creds_dicts: list[dict], state_key="drive_key_idx", log=None):
        self.rot = RotatingKeys(
            list(range(len(creds_dicts))),
            state_key,
            on_rotate=lambda i, _: log and log(f"ğŸ” Drive í‚¤ ì „í™˜ â†’ #{i+1}"),
            treat_as_strings=False,  # << ì¤‘ìš”: ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ë¬¸ìì—´ ì·¨ê¸‰í•˜ì§€ ì•ŠìŒ
        )
        self.creds_dicts = creds_dicts or []
        if not self.creds_dicts:
            raise RuntimeError("Drive ì„œë¹„ìŠ¤ ê³„ì • í‚¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        self.service = None
        self._build()
        self.log = log
    def _build(self):
        self.service = _build_drive_service_from_creds_dict(self.creds_dicts[self.rot.idx])
    def _rotate_and_rebuild(self):
        self.rot.rotate()
        self._build()
    def execute(self, fn, tries_per_key=2):
        attempts = 0
        max_attempts = len(self.creds_dicts) if self.creds_dicts else 1
        while attempts < max_attempts:
            try:
                return with_retry(lambda: fn(self.service), tries=tries_per_key, backoff=1.6)
            except Exception:
                if attempts < max_attempts - 1:
                    self._rotate_and_rebuild()
                    attempts += 1
                    continue
                raise

def drive_create_folder(rd: RotatingDrive, name: str, parent_id: str) -> str:
    def _create(svc):
        meta = {"name": name, "mimeType": "application/vnd.google-apps.folder", "parents": [parent_id]}
        return svc.files().create(body=meta, fields="id").execute()["id"]
    return rd.execute(_create)

def drive_upload_file(rd: RotatingDrive, folder_id: str, local_path: str, mime_type: str | None = None) -> dict:
    filename = os.path.basename(local_path)
    def _upload(svc):
        meta = {"name": filename, "parents": [folder_id]}
        media = MediaFileUpload(local_path, mimetype=mime_type, resumable=False)
        return svc.files().create(body=meta, media_body=media, fields="id, name, mimeType, webViewLink, webContentLink, size, createdTime").execute()
    return rd.execute(_upload)

def drive_list_folders(rd: RotatingDrive, parent_id: str) -> list[dict]:
    items = []
    page_token = None
    query = f"'{parent_id}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false"
    def _list(svc, token):
        return svc.files().list(q=query, spaces="drive", fields="nextPageToken, files(id, name, createdTime)", pageToken=token, orderBy="name").execute()
    while True:
        resp = rd.execute(lambda svc: _list(svc, page_token))
        items.extend(resp.get("files", []))
        page_token = resp.get("nextPageToken")
        if not page_token:
            break
    return items

def drive_list_files_in_folder(rd: RotatingDrive, folder_id: str) -> list[dict]:
    items, page_token = [], None
    query = f"'{folder_id}' in parents and trashed=false"
    def _list(svc, token):
        return svc.files().list(q=query, spaces="drive", fields="nextPageToken, files(id, name, mimeType, size, webViewLink, webContentLink, createdTime)", pageToken=token, orderBy="name").execute()
    while True:
        resp = rd.execute(lambda svc: _list(svc, page_token))
        items.extend(resp.get("files", []))
        page_token = resp.get("nextPageToken")
        if not page_token:
            break
    return items

# ===================== ìœ í‹¸: ID/URL =====================
def extract_video_id_one(s: str):
    s = (s or "").strip()
    if not s: return None
    if _YT_ID_RE.match(s): return s
    try:
        u = urlparse(s)
    except Exception:
        return None
    q = parse_qs(u.query or "")
    if u.path == "/watch" and "v" in q:
        v = q.get("v", [""])[0]
        return v if _YT_ID_RE.match(v) else None
    if u.netloc.endswith("youtu.be"):
        v = u.path.strip("/")
        return v if _YT_ID_RE.match(v) else None
    if "youtube.com" in u.netloc and "/embed/" in u.path:
        v = u.path.split("/embed/")[-1].split("/")[0]
        return v if _YT_ID_RE.match(v) else None
    if "youtube.com" in u.netloc and u.path.startswith("/shorts/"):
        v = u.path.split("/shorts/")[-1].split("/")[0]
        return v if _YT_ID_RE.match(v) else None
    return None

def extract_video_ids_from_text(text: str):
    ids = []
    for line in (text or "").splitlines():
        vid = extract_video_id_one(line)
        if vid and vid not in ids:
            ids.append(vid)
    return ids

# ===================== ì§ë ¬í™”/ìƒ˜í”Œë§ =====================
def sample_max_5000(df: pd.DataFrame) -> pd.DataFrame:
    n = len(df)
    if n <= 5000:
        return df.copy().reset_index(drop=True)
    try:
        return df.nlargest(5000, "likeCount").reset_index(drop=True)
    except Exception:
        return df.head(5000).reset_index(drop=True)

def serialize_comments_for_llm(df: pd.DataFrame, max_rows=1500, max_chars_per_comment=280, max_total_chars=450_000):
    if df is None or df.empty:
        return "", 0, 0
    try:
        df2 = df.nlargest(max_rows, "likeCount") if "likeCount" in df.columns else df.head(max_rows)
    except Exception:
        df2 = df.head(max_rows)
    lines, total = [], 0
    for _, r in df2.iterrows():
        is_reply = "R" if int(r.get("isReply", 0) or 0) == 1 else "T"
        author = str(r.get("author", "") or "").replace("\n", " ")
        likec = int(r.get("likeCount", 0) or 0)
        text = str(r.get("text", "") or "").replace("\n", " ")
        if len(text) > max_chars_per_comment:
            text = text[:max_chars_per_comment] + "â€¦"
        line = f"[{is_reply}|â™¥{likec}] {author}: {text}"
        if total + len(line) + 1 > max_total_chars:
            break
        lines.append(line)
        total += len(line) + 1
    return "\n".join(lines), len(lines), total

# ===================== YouTube API í•¨ìˆ˜ =====================
def yt_search_videos(rt, keyword, max_results, order="relevance", published_after=None, published_before=None, log=None):
    video_ids, token = [], None
    while len(video_ids) < max_results:
        params = dict(q=keyword, part="id", type="video", order=order, maxResults=min(50, max_results - len(video_ids)))
        if published_after: params["publishedAfter"] = published_after
        if published_before: params["publishedBefore"] = published_before
        if token: params["pageToken"] = token
        resp = rt.execute(lambda s: s.search().list(**params))
        for it in resp.get("items", []):
            vid = it["id"]["videoId"]
            if vid not in video_ids: video_ids.append(vid)
        token = resp.get("nextPageToken")
        if not token: break
        if log: log(f"ê²€ìƒ‰ ì§„í–‰: {len(video_ids)}ê°œ")
        time.sleep(0.35)
    return video_ids

def yt_video_statistics(rt, video_ids, log=None):
    rows = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i + 50]
        resp = rt.execute(lambda s: s.videos().list(part="statistics,snippet,contentDetails", id=",".join(batch)))
        for item in resp.get("items", []):
            stats = item.get("statistics", {})
            snip = item.get("snippet", {})
            cont = item.get("contentDetails", {})
            dur_iso = cont.get("duration", "")
            def _dsec(dur: str):
                if not dur or not dur.startswith("P"): return None
                h = re.search(r"(\d+)H", dur); m = re.search(r"(\d+)M", dur); s = re.search(r"(\d+)S", dur)
                return (int(h.group(1)) if h else 0) * 3600 + (int(m.group(1)) if m else 0) * 60 + (int(s.group(1)) if s else 0)
            dur_sec = _dsec(dur_iso)
            short_type = "Shorts" if (dur_sec is not None and dur_sec <= 60) else "Clip"
            vid_id = item.get("id")
            rows.append({
                "video_id": vid_id,
                "video_url": f"https://www.youtube.com/watch?v={vid_id}",
                "title": snip.get("title", ""),
                "channelTitle": snip.get("channelTitle", ""),
                "publishedAt": snip.get("publishedAt", ""),
                "duration": dur_iso,
                "shortType": short_type,
                "viewCount": int(stats.get("viewCount", 0) or 0),
                "likeCount": int(stats.get("likeCount", 0) or 0),
                "commentCount": int(stats.get("commentCount", 0) or 0),
            })
        if log: log(f"í†µê³„ ë°°ì¹˜ {i // 50 + 1} ì™„ë£Œ")
        time.sleep(0.35)
    return rows

def yt_all_replies(rt, parent_id, video_id, title="", short_type="Clip", log=None, cap=None):
    replies, token = [], None
    while True:
        if cap is not None and len(replies) >= cap:
            return replies[:cap]
        params = dict(part="snippet", parentId=parent_id, maxResults=100, pageToken=token, textFormat="plainText")
        try:
            resp = rt.execute(lambda s: s.comments().list(**params))
        except HttpError as e:
            if log: log(f"[ì˜¤ë¥˜] replies {video_id}/{parent_id}: {e}")
            break
        for c in resp.get("items", []):
            sn = c["snippet"]
            replies.append({
                "video_id": video_id, "video_title": title, "shortType": short_type,
                "comment_id": c.get("id", ""), "parent_id": parent_id, "isReply": 1,
                "author": sn.get("authorDisplayName", ""),
                "text": sn.get("textDisplay", "") or "",
                "publishedAt": sn.get("publishedAt", ""),
                "likeCount": int(sn.get("likeCount", 0) or 0),
            })
            if cap is not None and len(replies) >= cap:
                return replies[:cap]
        token = resp.get("nextPageToken")
        if not token: break
        time.sleep(0.25)
    return replies

def yt_all_comments_sync(rt, video_id, title="", short_type="Clip", include_replies=True, log=None,
                    max_per_video: int | None = None):
    rows, token = [], None
    while True:
        if max_per_video is not None and len(rows) >= max_per_video:
            return rows[:max_per_video]
        params = dict(part="snippet,replies", videoId=video_id, maxResults=100, pageToken=token, textFormat="plainText")
        try:
            resp = rt.execute(lambda s: s.commentThreads().list(**params))
        except HttpError as e:
            if log: log(f"[ì˜¤ë¥˜] commentThreads {video_id}: {e}")
            break

        for it in resp.get("items", []):
            top = it["snippet"]["topLevelComment"]["snippet"]
            thread_id = it["snippet"]["topLevelComment"]["id"]
            total_replies = int(it["snippet"].get("totalReplyCount", 0) or 0)
            rows.append({
                "video_id": video_id, "video_title": title, "shortType": short_type,
                "comment_id": thread_id, "parent_id": "", "isReply": 0,
                "author": top.get("authorDisplayName", ""),
                "text": top.get("textDisplay", "") or "",
                "publishedAt": top.get("publishedAt", ""),
                "likeCount": int(top.get("likeCount", 0) or 0),
            })
            if include_replies and total_replies > 0:
                cap = None
                if max_per_video is not None:
                    cap = max(0, max_per_video - len(rows))
                if cap == 0:
                    return rows[:max_per_video]
                rows.extend(yt_all_replies(rt, thread_id, video_id, title, short_type, log, cap=cap))
                if max_per_video is not None and len(rows) >= max_per_video:
                    return rows[:max_per_video]

        token = resp.get("nextPageToken")
        if not token: break
        if log: log(f"  ëŒ“ê¸€ í˜ì´ì§€ ì§„í–‰, ëˆ„ê³„ {len(rows)}")
        time.sleep(0.25)
    return rows

def parallel_collect_comments(video_list, rt_keys, include_replies, max_total_comments, max_per_video, log_callback, prog_callback):
    all_comments = []
    total_videos = len(video_list)
    collected_videos = 0
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {
            executor.submit(
                yt_all_comments_sync,
                RotatingYouTube(rt_keys),
                vid_info["video_id"],
                vid_info.get("title", ""),
                vid_info.get("shortType", "Clip"),
                include_replies,
                None,
                max_per_video
            ): vid_info for vid_info in video_list
        }
        for future in as_completed(futures):
            vid_info = futures[future]
            try:
                comments = future.result()
                all_comments.extend(comments)
                collected_videos += 1
                if log_callback: log_callback(f"âœ… [{collected_videos}/{total_videos}] {vid_info.get('title','')} - {len(comments):,}ê°œ ìˆ˜ì§‘")
                if prog_callback: prog_callback(collected_videos / total_videos)
            except Exception as e:
                if log_callback: log_callback(f"âŒ [{collected_videos+1}/{total_videos}] {vid_info.get('title','')} - ì‹¤íŒ¨: {e}")
                collected_videos += 1
                if prog_callback: prog_callback(collected_videos / total_videos)
            if len(all_comments) >= max_total_comments:
                if log_callback: log_callback(f"ìµœëŒ€ ìˆ˜ì§‘ í•œë„({max_total_comments:,}ê°œ) ë„ë‹¬, ì¤‘ë‹¨")
                break
    return all_comments[:max_total_comments]

# ===================== ì„¸ì…˜ ìƒíƒœ =====================
def ensure_state():
    defaults = dict(
        focus_step=1,
        last_keyword="",
        # ì‹¬í”Œ
        s_query="", s_df_comments=None, s_df_analysis=None, s_df_stats=None,
        s_serialized_sample="", s_result_text="",
        s_history=[], s_preset="ìµœê·¼ 1ë…„",
        # ê³ ê¸‰
        mode="ê²€ìƒ‰ ëª¨ë“œ",
        df_stats=None, selected_ids=[],
        df_comments=None, df_analysis=None,
        adv_serialized_sample="", adv_result_text="",
        adv_followups=[], adv_history=[],
        # ì…ë ¥ê°’
        simple_follow_q="", adv_follow_q="",
    )
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v
ensure_state()

# ===================== íˆìŠ¤í† ë¦¬ â†’ ì»¨í…ìŠ¤íŠ¸ =====================
def build_history_context(pairs: list[tuple[str, str]]) -> str:
    if not pairs:
        return ""
    lines = []
    for i, (q, a) in enumerate(pairs, 1):
        lines.append(f"[ì´ì „ Q{i}]: {q}")   # â† ì—¬ê¸° ë‹«ëŠ” ê´„í˜¸ë¥¼ } ë¡œ
        lines.append(f"[ì´ì „ A{i}]: {a}")   # â† ì´ ì¤„ë„ ë™ì¼ íŒ¨í„´
    return "\n".join(lines)


# ===================== ì‹œê°í™” ë„êµ¬(ì €ì¥ìš©) =====================
def _fig_keyword_bubble(df_comments) -> go.Figure | None:
    try:
        custom_stopwords = {
            "ì•„","íœ´","ì•„ì´êµ¬","ì•„ì´ì¿ ","ì•„ì´ê³ ","ì–´","ë‚˜","ìš°ë¦¬","ì €í¬","ë”°ë¼","ì˜í•´","ì„","ë¥¼",
            "ì—","ì˜","ê°€","ìœ¼ë¡œ","ë¡œ","ì—ê²Œ","ë¿ì´ë‹¤","ì˜ê±°í•˜ì—¬","ê·¼ê±°í•˜ì—¬","ì…ê°í•˜ì—¬","ê¸°ì¤€ìœ¼ë¡œ",
            "ê·¸ëƒ¥","ëŒ“ê¸€","ì˜ìƒ","ì˜¤ëŠ˜","ì´ì œ","ë­","ì§„ì§œ","ì •ë§","ë¶€ë¶„","ìš”ì¦˜","ì œë°œ","ì™„ì „",
            "ê·¸ê²Œ","ì¼ë‹¨","ëª¨ë“ ","ìœ„í•´","ëŒ€í•œ","ìˆì§€","ì´ìœ ","ê³„ì†","ì‹¤ì œ","ìœ íŠœë¸Œ","ì´ë²ˆ","ê°€ì¥","ë“œë¼ë§ˆ",
        }
        stopset = set(korean_stopwords); stopset.update(custom_stopwords)
        query_kw = (st.session_state.get("s_query")
                    or st.session_state.get("last_keyword")
                    or st.session_state.get("adv_analysis_keyword")
                    or "").strip()
        if query_kw:
            tokens_q = kiwi.tokenize(query_kw, normalize_coda=True)
            query_words = [t.form for t in tokens_q if t.tag in ("NNG","NNP") and len(t.form) > 1]
            stopset.update(query_words)

        texts = " ".join(df_comments["text"].astype(str).tolist())
        tokens = kiwi.tokenize(texts, normalize_coda=True)
        words = [t.form for t in tokens if t.tag in ("NNG","NNP") and len(t.form) > 1 and t.form not in stopset]
        counter = Counter(words)
        if len(counter) == 0:
            return None
        df_kw = pd.DataFrame(counter.most_common(30), columns=["word", "count"])
        df_kw["label"] = df_kw["word"] + "<br>" + df_kw["count"].astype(str)
        df_kw["scaled"] = np.sqrt(df_kw["count"])
        data_for_pack = [{"id": w, "datum": s} for w, s in zip(df_kw["word"], df_kw["scaled"])]
        circles = circlify.circlify(data_for_pack, show_enclosure=False,
                                    target_enclosure=circlify.Circle(x=0, y=0, r=1))
        pos = {c.ex["id"]: (c.x, c.y, c.r) for c in circles if "id" in c.ex}
        df_kw["x"] = df_kw["word"].map(lambda w: pos[w][0])
        df_kw["y"] = df_kw["word"].map(lambda w: pos[w][1])
        df_kw["r"] = df_kw["word"].map(lambda w: pos[w][2])
        min_size, max_size = 10, 22
        s_min, s_max = df_kw["scaled"].min(), df_kw["scaled"].max()
        df_kw["font_size"] = df_kw["scaled"].apply(
            lambda s: int(min_size + (s - s_min) / (s_max - s_min) * (max_size - min_size)) if s_max > s_min else 14
        )
        fig_kw = go.Figure()
        palette = px.colors.sequential.Blues
        df_kw["color_idx"] = df_kw["scaled"].apply(lambda s: int((s - s_min) / max(s_max - s_min, 1) * (len(palette) - 1)))
        for _, row in df_kw.iterrows():
            color = palette[int(row["color_idx"])]
            fig_kw.add_shape(
                type="circle", xref="x", yref="y",
                x0=row["x"] - row["r"], y0=row["y"] - row["r"],
                x1=row["x"] + row["r"], y1=row["y"] + row["r"],
                line=dict(width=0), fillcolor=color, opacity=0.88, layer="below"
            )
        fig_kw.add_trace(go.Scatter(
            x=df_kw["x"], y=df_kw["y"], mode="text",
            text=df_kw["label"], textposition="middle center",
            textfont=dict(color="white", size=df_kw["font_size"].tolist()),
            hovertext=df_kw["word"] + " (" + df_kw["count"].astype(str) + ")",
            hovertemplate="%{hovertext}<extra></extra>",
        ))
        fig_kw.update_xaxes(visible=False, range=[-1.05, 1.05])
        fig_kw.update_yaxes(visible=False, range=[-1.05, 1.05], scaleanchor="x", scaleratio=1)
        fig_kw.update_layout(title="Top30 í‚¤ì›Œë“œ ë²„ë¸”", plot_bgcolor="rgba(0,0,0,0)", margin=dict(l=0, r=0, t=40, b=0))
        return fig_kw
    except Exception:
        return None

def _fig_time_series(df_comments, scope_label="(KST ê¸°ì¤€)"):
    df_time = df_comments.copy()
    df_time["publishedAt"] = pd.to_datetime(df_time["publishedAt"], errors="coerce", utc=True).dt.tz_convert("Asia/Seoul")
    df_time = df_time.dropna(subset=["publishedAt"])
    if df_time.empty:
        return None
    span_hours = (df_time["publishedAt"].max() - df_time["publishedAt"].min()).total_seconds()/3600.0
    rule = "H" if span_hours <= 48 else "D"
    label = "ì‹œê°„ë³„" if rule == "H" else "ì¼ìë³„"
    ts = df_time.resample(rule, on="publishedAt").size().reset_index(name="count")
    fig_ts = px.line(ts, x="publishedAt", y="count", markers=True, title=f"{label} ëŒ“ê¸€ëŸ‰ ì¶”ì´ {scope_label}")
    return fig_ts

def _fig_top_videos(df_stats):
    if df_stats is None or df_stats.empty:
        return None
    top_vids = df_stats.sort_values(by="commentCount", ascending=False).head(10).copy()
    top_vids["title_short"] = top_vids["title"].apply(lambda t: t[:20] + "â€¦" if isinstance(t, str) and len(t) > 20 else t)
    fig_vids = px.bar(top_vids, x="commentCount", y="title_short", orientation="h", text="commentCount",
                      title="Top10 ì˜ìƒ ëŒ“ê¸€ìˆ˜")
    return fig_vids

def _fig_top_authors(df_comments):
    if df_comments is None or df_comments.empty:
        return None
    top_authors = (df_comments.groupby("author").size().reset_index(name="count").sort_values(by="count", ascending=False).head(10))
    if top_authors.empty:
        return None
    fig_auth = px.bar(top_authors, x="count", y="author", orientation="h", text="count", title="Top10 ëŒ“ê¸€ ì‘ì„±ì í™œë™ëŸ‰")
    return fig_auth

def _save_fig_png(fig: go.Figure, path: str):
    if fig is None:
        return False
    try:
        fig.write_image(path, format="png", scale=2)
        return True
    except Exception:
        return False

# ===================== ì„¸ì…˜ ì €ì¥/ZIP (ë¡œì»¬ + Drive ì—…ë¡œë“œ) =====================
def _save_df_csv(df: pd.DataFrame, path: str):
    if df is None or (hasattr(df, "empty") and df.empty): return
    df.to_csv(path, index=False, encoding="utf-8-sig")

def _slugify_filename(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"[^\w\-]+", "", s)
    if not s:
        s = "no_kw"
    return s[:60]

def _build_session_name() -> str:
    # ì´ë¦„ í¬ë§·: ê²€ìƒ‰ì–´_yyyy-mm-dd-hh:mm_ê²€ìƒ‰ê¸°ê°„ (KST)
    kw = (st.session_state.get("s_query") or st.session_state.get("last_keyword") or "").strip() or "no_kw"
    preset = (st.session_state.get("s_preset") or "ìµœê·¼ 1ë…„").replace(" ", "")
    now_kst = datetime.now(_kst_tz()).strftime("%Y-%m-%d-%H:%M")
    return f"{_slugify_filename(kw)}_{now_kst}_{preset}"

def _write_ai_texts(outdir: str):
    # ai_simple.md / ai_advanced.md
    simple_txt = st.session_state.get("s_result_text", "")
    adv_txt = st.session_state.get("adv_result_text", "")
    if simple_txt:
        with open(os.path.join(outdir, "ai_simple.md"), "w", encoding="utf-8") as f:
            f.write(simple_txt)
    if adv_txt:
        with open(os.path.join(outdir, "ai_advanced.md"), "w", encoding="utf-8") as f:
            f.write(adv_txt)

def _write_viz_pngs(outdir: str):
    # í˜„ì¬ ì„¸ì…˜ ë°ì´í„°ì—ì„œ ê·¸ë¦¼ ì¬ìƒì„± â†’ PNG ì €ì¥
    df_comments_s = st.session_state.get("s_df_comments")
    df_stats_s = st.session_state.get("s_df_stats")
    df_comments_a = st.session_state.get("df_comments")
    df_stats_a = st.session_state.get("df_stats")

    # ì‹¬í”Œ ê¸°ì¤€(ìˆìœ¼ë©´ ìš°ì„ )
    dfc = df_comments_s if (df_comments_s is not None and not df_comments_s.empty) else st.session_state.get("df_comments")
    dfs = df_stats_s if (df_stats_s is not None and not df_stats_s.empty) else st.session_state.get("df_stats")

    figs = {
        "viz_keyword_bubble.png": _fig_keyword_bubble(dfc) if dfc is not None and not dfc.empty else None,
        "viz_time_series.png": _fig_time_series(dfc) if dfc is not None and not dfc.empty else None,
        "viz_top_videos.png": _fig_top_videos(dfs) if dfs is not None and not dfs.empty else None,
        "viz_top_authors.png": _fig_top_authors(dfc) if dfc is not None and not dfc.empty else None,
    }
    for fname, fig in figs.items():
        _save_fig_png(fig, os.path.join(outdir, fname))

def save_current_session(name_prefix: str | None = None):
    # ìƒˆ í¬ë§·ìœ¼ë¡œ ì„¸ì…˜ ì´ë¦„ ìƒì„±
    sess_name = _build_session_name()
    outdir = os.path.join(SESS_DIR, sess_name)
    os.makedirs(outdir, exist_ok=True)

    # ë©”íƒ€/QA ì €ì¥
    qa_data = {
        "simple_history": st.session_state.get("s_history", []),
        "adv_history": st.session_state.get("adv_history", []),
        "simple_query": st.session_state.get("s_query",""),
        "last_keyword": st.session_state.get("last_keyword",""),
        "preset": st.session_state.get("s_preset",""),
        "saved_at_kst": datetime.now(_kst_tz()).strftime("%Y-%m-%d %H:%M:%S")
    }
    with open(os.path.join(outdir, "qa.json"), "w", encoding="utf-8") as f:
        json.dump(qa_data, f, ensure_ascii=False, indent=2)

    # CSV ì €ì¥
    _save_df_csv(st.session_state.get("s_df_comments"), os.path.join(outdir, "simple_comments_full.csv"))
    _save_df_csv(st.session_state.get("s_df_analysis"), os.path.join(outdir, "simple_comments_sample.csv"))
    _save_df_csv(st.session_state.get("s_df_stats"), os.path.join(outdir, "simple_videos.csv"))
    _save_df_csv(st.session_state.get("df_comments"), os.path.join(outdir, "adv_comments_full.csv"))
    _save_df_csv(st.session_state.get("df_analysis"), os.path.join(outdir, "adv_comments_sample.csv"))
    _save_df_csv(st.session_state.get("df_stats"), os.path.join(outdir, "adv_videos.csv"))

    # AI í…ìŠ¤íŠ¸ + ì‹œê°í™” PNG ì €ì¥
    _write_ai_texts(outdir)
    _write_viz_pngs(outdir)

    # Drive ì—…ë¡œë“œ (ì„¸ì…˜ í´ë” ìƒì„± í›„ ì „ì²´ ì—…ë¡œë“œ)
    drive_folder_id = None
    uploaded = []
    if GDRIVE_PARENT_FOLDER_ID and GDRIVE_KEYS:
        try:
            rd = RotatingDrive(GDRIVE_KEYS, log=lambda m: st.write(m))
            drive_folder_id = drive_create_folder(rd, sess_name, GDRIVE_PARENT_FOLDER_ID)
            # ì—…ë¡œë“œ
            mimemap = {
                ".csv": "text/csv",
                ".json": "application/json",
                ".md": "text/markdown",
                ".png": "image/png",
                ".zip": "application/zip"
            }
            for fn in sorted(os.listdir(outdir)):
                p = os.path.join(outdir, fn)
                if not os.path.isfile(p):
                    continue
                ext = os.path.splitext(fn)[1].lower()
                mime = mimemap.get(ext, "application/octet-stream")
                info = drive_upload_file(rd, drive_folder_id, p, mime)
                uploaded.append(info)
            # manifest.json ì—…ë¡œë“œ
            manifest = {
                "session_name": sess_name,
                "parent_folder_id": GDRIVE_PARENT_FOLDER_ID,
                "drive_folder_id": drive_folder_id,
                "uploaded": uploaded,
                "created_kst": datetime.now(_kst_tz()).strftime("%Y-%m-%d %H:%M:%S")
            }
            man_local = os.path.join(outdir, "manifest.json")
            with open(man_local, "w", encoding="utf-8") as f:
                json.dump(manifest, f, ensure_ascii=False, indent=2)
            drive_upload_file(rd, drive_folder_id, man_local, "application/json")
        except Exception as e:
            st.warning(f"Drive ì—…ë¡œë“œ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")

    return sess_name, drive_folder_id

def list_sessions_local():
    if not os.path.exists(SESS_DIR): return []
    return sorted([d for d in os.listdir(SESS_DIR) if os.path.isdir(os.path.join(SESS_DIR,d))], reverse=True)

def zip_session(sess_name: str):
    sess_path = os.path.join(SESS_DIR, sess_name)
    zip_path = os.path.join(SESS_DIR, f"{sess_name}.zip")
    shutil.make_archive(zip_path.replace(".zip",""), 'zip', sess_path)
    return zip_path

# ===================== ì‹œê°í™”/ë‹¤ìš´ë¡œë“œ(í™”ë©´ ë Œë”) =====================
def render_keyword_bubble(s_df_comments):
    st.subheader("â‘  í‚¤ì›Œë“œ ë²„ë¸”")
    try:
        custom_stopwords = {
            "ì•„","íœ´","ì•„ì´êµ¬","ì•„ì´ì¿ ","ì•„ì´ê³ ","ì–´","ë‚˜","ìš°ë¦¬","ì €í¬","ë”°ë¼","ì˜í•´","ì„","ë¥¼",
            "ì—","ì˜","ê°€","ìœ¼ë¡œ","ë¡œ","ì—ê²Œ","ë¿ì´ë‹¤","ì˜ê±°í•˜ì—¬","ê·¼ê±°í•˜ì—¬","ì…ê°í•˜ì—¬","ê¸°ì¤€ìœ¼ë¡œ",
            "ê·¸ëƒ¥","ëŒ“ê¸€","ì˜ìƒ","ì˜¤ëŠ˜","ì´ì œ","ë­","ì§„ì§œ","ì •ë§","ë¶€ë¶„","ìš”ì¦˜","ì œë°œ","ì™„ì „",
            "ê·¸ê²Œ","ì¼ë‹¨","ëª¨ë“ ","ìœ„í•´","ëŒ€í•œ","ìˆì§€","ì´ìœ ","ê³„ì†","ì‹¤ì œ","ìœ íŠœë¸Œ","ì´ë²ˆ","ê°€ì¥","ë“œë¼ë§ˆ",
        }
        stopset = set(korean_stopwords); stopset.update(custom_stopwords)

        # ğŸ”‘ ê²€ìƒ‰ì–´ ë¶ˆìš©ì–´ ì¶”ê°€
        query_kw = (st.session_state.get("s_query") 
                    or st.session_state.get("last_keyword") 
                    or st.session_state.get("adv_analysis_keyword") 
                    or "").strip()
        if query_kw:
            tokens_q = kiwi.tokenize(query_kw, normalize_coda=True)
            query_words = [t.form for t in tokens_q if t.tag in ("NNG","NNP") and len(t.form) > 1]
            stopset.update(query_words)

        texts = " ".join(s_df_comments["text"].astype(str).tolist())
        tokens = kiwi.tokenize(texts, normalize_coda=True)
        words = [t.form for t in tokens if t.tag in ("NNG","NNP") and len(t.form) > 1 and t.form not in stopset]
        counter = Counter(words)
        if len(counter) == 0:
            st.info("í‘œì‹œí•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤(ë¶ˆìš©ì–´ ì œê±° í›„ ë‚¨ì€ ë‹¨ì–´ ì—†ìŒ).")
            return
        df_kw = pd.DataFrame(counter.most_common(30), columns=["word", "count"])
        df_kw["label"] = df_kw["word"] + "<br>" + df_kw["count"].astype(str)
        df_kw["scaled"] = np.sqrt(df_kw["count"])
        data_for_pack = [{"id": w, "datum": s} for w, s in zip(df_kw["word"], df_kw["scaled"])]
        circles = circlify.circlify(data_for_pack, show_enclosure=False,
                                    target_enclosure=circlify.Circle(x=0, y=0, r=1))
        pos = {c.ex["id"]: (c.x, c.y, c.r) for c in circles if "id" in c.ex}
        df_kw["x"] = df_kw["word"].map(lambda w: pos[w][0])
        df_kw["y"] = df_kw["word"].map(lambda w: pos[w][1])
        df_kw["r"] = df_kw["word"].map(lambda w: pos[w][2])
        min_size, max_size = 10, 22
        s_min, s_max = df_kw["scaled"].min(), df_kw["scaled"].max()
        df_kw["font_size"] = df_kw["scaled"].apply(
            lambda s: int(min_size + (s - s_min) / (s_max - s_min) * (max_size - min_size)) if s_max > s_min else 14
        )
        fig_kw = go.Figure()
        palette = px.colors.sequential.Blues
        df_kw["color_idx"] = df_kw["scaled"].apply(lambda s: int((s - s_min) / max(s_max - s_min, 1) * (len(palette) - 1)))
        for _, row in df_kw.iterrows():
            color = palette[int(row["color_idx"])]
            fig_kw.add_shape(
                type="circle", xref="x", yref="y",
                x0=row["x"] - row["r"], y0=row["y"] - row["r"],
                x1=row["x"] + row["r"], y1=row["y"] + row["r"],
                line=dict(width=0), fillcolor=color, opacity=0.88, layer="below"
            )
        fig_kw.add_trace(go.Scatter(
            x=df_kw["x"], y=df_kw["y"], mode="text",
            text=df_kw["label"], textposition="middle center",
            textfont=dict(color="white", size=df_kw["font_size"].tolist()),
            hovertext=df_kw["word"] + " (" + df_kw["count"].astype(str) + ")",
            hovertemplate="%{hovertext}<extra></extra>",
        ))
        fig_kw.update_xaxes(visible=False, range=[-1.05, 1.05])
        fig_kw.update_yaxes(visible=False, range=[-1.05, 1.05], scaleanchor="x", scaleratio=1)
        fig_kw.update_layout(title="Top30 í‚¤ì›Œë“œ ë²„ë¸”", plot_bgcolor="rgba(0,0,0,0)", margin=dict(l=0, r=0, t=40, b=0))
        st.plotly_chart(fig_kw, use_container_width=True)
    except Exception as e:
        st.info(f"í‚¤ì›Œë“œ ë¶„ì„ ë¶ˆê°€: {e}")

def render_quant_viz(df_comments, df_stats, scope_label="(KST ê¸°ì¤€)"):
    if df_comments is not None and not df_comments.empty:
        with st.expander("ğŸ“Š ì •ëŸ‰ ìš”ì•½", expanded=True):
            col1, col2 = st.columns(2)
            with col1:
                with st.container(border=True):
                    render_keyword_bubble(df_comments)
            with col2:
                with st.container(border=True):
                    st.subheader("â‘¡ ì‹œì ë³„ ëŒ“ê¸€ëŸ‰ ë³€ë™ ì¶”ì´")
                    df_time = df_comments.copy()
                    df_time["publishedAt"] = (
                        pd.to_datetime(df_time["publishedAt"], errors="coerce", utc=True).dt.tz_convert("Asia/Seoul")
                    )
                    df_time = df_time.dropna(subset=["publishedAt"])
                    if not df_time.empty:
                        span_hours = (df_time["publishedAt"].max() - df_time["publishedAt"].min()).total_seconds()/3600.0
                        rule = "H" if span_hours <= 48 else "D"
                        label = "ì‹œê°„ë³„" if rule == "H" else "ì¼ìë³„"
                        ts = df_time.resample(rule, on="publishedAt").size().reset_index(name="count")
                        fig_ts = px.line(ts, x="publishedAt", y="count", markers=True,
                                         title=f"{label} ëŒ“ê¸€ëŸ‰ ì¶”ì´ {scope_label}")
                        st.plotly_chart(fig_ts, use_container_width=True)
                    else:
                        st.info("ëŒ“ê¸€ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            if df_stats is not None and not df_stats.empty:
                col3, col4 = st.columns(2)
                with col3:
                    with st.container(border=True):
                        st.subheader("â‘¢ Top10 ì˜ìƒ ëŒ“ê¸€ìˆ˜")
                        top_vids = df_stats.sort_values(by="commentCount", ascending=False).head(10).copy()
                        top_vids["title_short"] = top_vids.apply(
                            lambda r: f'<a href="https://www.youtube.com/watch?v={r["video_id"]}" target="_blank" '
                                      f'style="color:black; text-decoration:none;">'
                                      f'{r["title"][:20] + "â€¦" if len(r["title"]) > 20 else r["title"]}</a>',
                            axis=1
                        )
                        fig_vids = px.bar(top_vids, x="commentCount", y="title_short",
                                          orientation="h", text="commentCount",
                                          hover_data={"title": True, "video_id": False})
                        st.plotly_chart(fig_vids, use_container_width=True)
                with col4:
                    with st.container(border=True):
                        st.subheader("â‘£ ëŒ“ê¸€ ì‘ì„±ì í™œë™ëŸ‰ Top10")
                        top_authors = (
                            df_comments.groupby("author").size().reset_index(name="count")
                            .sort_values(by="count", ascending=False).head(10)
                        )
                        fig_auth = px.bar(top_authors, x="count", y="author", orientation="h", text="count",
                                          title="Top10 ëŒ“ê¸€ ì‘ì„±ì í™œë™ëŸ‰")
                        st.plotly_chart(fig_auth, use_container_width=True)
                        user_counts = df_comments.groupby("author").size()
                        total_users = len(user_counts)
                        active_users = user_counts[user_counts >= 3].count()
                        perc = (active_users / total_users * 100) if total_users > 0 else 0
                        st.markdown(f"**3íšŒ ì´ìƒ ëŒ“ê¸€ ì‘ì„± ìœ ì € ë¹„ì¤‘:** {active_users:,}ëª… / {total_users:,}ëª… ({perc:.1f}%)")
            with st.container(border=True):
                st.subheader("â‘¤ ëŒ“ê¸€ ì¢‹ì•„ìš” Top10")
                top_comments = df_comments.sort_values(by="likeCount", ascending=False).head(10)
                for _, row in top_comments.iterrows():
                    url = f"https://www.youtube.com/watch?v={row['video_id']}"
                    st.markdown(
                        f"<div style='margin-bottom:15px;'>"
                        f"<b>{row['likeCount']} ğŸ‘</b> â€” {row['author']}<br>"
                        f"<span style='font-size:14px;'>â–¶ï¸ <a href='{url}' target='_blank' style='color:black; text-decoration:none;'>"
                        f"{row.get('video_title','(ì œëª©ì—†ìŒ)')}</a></span><br>"
                        f"> {row['text'][:150]}{'â€¦' if len(row['text'])>150 else ''}"
                        f"</div>",
                        unsafe_allow_html=True
                    )

def render_downloads(df_comments, df_analysis, df_stats, prefix="simple"):
    if df_comments is not None and not df_comments.empty:
        st.markdown("---")
        st.subheader("â¬‡ï¸ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
        csv_full = df_comments.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        st.download_button(
            "ì „ì²´ ëŒ“ê¸€ (CSV)", data=csv_full,
            file_name=f"{prefix}_full_{len(df_comments)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv", key=f"{prefix}_dl_full_csv"
        )
        if df_analysis is not None:
            csv_sample = df_analysis.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
            st.download_button(
                "ë¶„ì„ìš© ìƒ˜í”Œ (CSV)", data=csv_sample,
                file_name=f"{prefix}_sample_{len(df_analysis)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv", key=f"{prefix}_dl_sample_csv"
            )
        if df_stats is not None and not df_stats.empty:
            df_videos = df_stats.copy()
            if "viewCount" in df_videos.columns:
                df_videos = df_videos.sort_values(by="viewCount", ascending=False).reset_index(drop=True)
            csv_videos = df_videos.applymap(clean_illegal).to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
            st.download_button(
                "ì˜ìƒëª©ë¡ (CSV)", data=csv_videos,
                file_name=f"{prefix}_videolist_{len(df_videos)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv", key=f"{prefix}_dl_videos_csv"
            )

# ===================== íƒ­ =====================
tab_simple, tab_advanced, tab_sessions = st.tabs(["ğŸŸ¢ ì‹¬í”Œ ëª¨ë“œ", "âš™ï¸ ê³ ê¸‰ ëª¨ë“œ", "ğŸ“‚ ì„¸ì…˜ ì•„ì¹´ì´ë¸Œ"])

# ===================== ì¶”ê°€ì§ˆë¬¸ í•¸ë“¤ëŸ¬ =====================
def handle_followup_simple():
    follow_q = (st.session_state.get("simple_follow_q") or "").strip()
    if not follow_q: return
    if not GEMINI_API_KEYS:
        st.error("Gemini API Keyê°€ ì—†ìŠµë‹ˆë‹¤."); return
    if not st.session_state.get("s_serialized_sample"):
        st.error("ë¶„ì„ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìˆ˜ì§‘/ë¶„ì„ ì‹¤í–‰."); return
    append_log("ì‹¬í”Œ-ì¶”ê°€", st.session_state.get("s_query",""), follow_q)  # no-op
    context_str = build_history_context(st.session_state.get("s_history", []))
    system_instruction = (
        "ë„ˆëŠ” ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ë¶„ì„í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. "
        "ì•„ë˜ëŠ” ì´ë¯¸ ì¶”ì¶œ/ê°€ê³µëœ ëŒ“ê¸€ ìƒ˜í”Œê³¼ ì´ì „ ì§ˆì˜ì‘ë‹µ íˆìŠ¤í† ë¦¬ë‹¤. "
        "ì´ì „ ë§¥ë½ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µì„ í•˜ë¼. "
        "ë°˜ë“œì‹œ ëª¨ë“  ëŒ“ê¸€ì„ ì½ê³  ë‹µë³€í•˜ë¼."
    )
    payload = ((context_str + "\n\n") if context_str else "") + (
        f"[í˜„ì¬ ì§ˆë¬¸]: {follow_q}\n"
        f"[ê¸°ê°„]: {st.session_state.get('s_preset','ìµœê·¼ 1ë…„')}\n\n"
        f"[ëŒ“ê¸€ ìƒ˜í”Œ]:\n{st.session_state['s_serialized_sample']}\n"
    )
    out = call_gemini_rotating(GEMINI_MODEL, GEMINI_API_KEYS, system_instruction, payload)
    st.session_state["s_history"].append((follow_q, out))
    st.session_state["simple_follow_q"] = ""
    st.success("ì¶”ê°€ ë¶„ì„ ì™„ë£Œ")

def handle_followup_advanced():
    adv_follow_q = (st.session_state.get("adv_follow_q") or "").strip()
    if not adv_follow_q: return
    if not GEMINI_API_KEYS:
        st.error("Gemini API Keyê°€ ì—†ìŠµë‹ˆë‹¤."); return
    df_analysis = st.session_state.get("df_analysis")
    if df_analysis is None or df_analysis.empty:
        st.error("ë¶„ì„ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìˆ˜ì§‘/ë¶„ì„ ì‹¤í–‰."); return
    append_log("ê³ ê¸‰-ì¶”ê°€", st.session_state.get("last_keyword",""), adv_follow_q)  # no-op
    a_text = st.session_state.get("adv_serialized_sample", "") or serialize_comments_for_llm(df_analysis)[0]
    context_str = build_history_context(st.session_state.get("adv_history", []))
    system_instruction = (
        "ë„ˆëŠ” ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ë¶„ì„í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. "
        "ì•„ë˜ëŠ” ì´ë¯¸ ì¶”ì¶œ/ê°€ê³µëœ ëŒ“ê¸€ ìƒ˜í”Œê³¼ ì´ì „ ì§ˆì˜ì‘ë‹µ íˆìŠ¤í† ë¦¬ë‹¤. "
        "ì´ì „ ë§¥ë½ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µì„ í•˜ë¼. "
        "ë°˜ë“œì‹œ ëª¨ë“  ëŒ“ê¸€ì„ ì½ê³  ë‹µë³€í•˜ë¼."
    )
    payload = ((context_str + "\n\n") if context_str else "") + (
        f"[í˜„ì¬ ì§ˆë¬¸]: {adv_follow_q}\n\n"
        f"[ëŒ“ê¸€ ìƒ˜í”Œ]:\n{a_text}\n"
    )
    out = call_gemini_rotating(GEMINI_MODEL, GEMINI_API_KEYS, system_instruction, payload)
    st.session_state["adv_followups"].append((adv_follow_q, out))
    st.session_state["adv_history"].append((adv_follow_q, out))
    st.session_state["adv_follow_q"] = ""
    st.success("ì¶”ê°€ ë¶„ì„ ì™„ë£Œ(ê³ ê¸‰)")

# ===================== 1) ì‹¬í”Œ ëª¨ë“œ =====================
with tab_simple:
    st.subheader("ìµœê·¼ ê¸°ê°„ ëŒ“ê¸€ ë°˜ì‘ â€” ë“œë¼ë§ˆ/ë°°ìš°ëª…ìœ¼ë¡œ ë°”ë¡œ ë¶„ì„")
    s_query = st.text_input("ë“œë¼ë§ˆ or ë°°ìš°ëª…", value=st.session_state.get("s_query", ""),
                            placeholder="í‚¤ì›Œë“œ ì…ë ¥", key="simple_query")
    preset_simple = st.radio(
        "ì—…ë¡œë“œ ê¸°ê°„ (KST)",
        ["ìµœê·¼ 12ì‹œê°„","ìµœê·¼ 24ì‹œê°„","ìµœê·¼ 48ì‹œê°„","ìµœê·¼ 1ì£¼ì¼","ìµœê·¼ 1ê°œì›”","ìµœê·¼ 6ê°œì›”",
         "ìµœê·¼ 1ë…„","ìµœê·¼ 2ë…„","ìµœê·¼ 3ë…„","ìµœê·¼ 4ë…„","ìµœê·¼ 5ë…„","ìµœê·¼ 10ë…„"],
        horizontal=True, key="simple_preset"
    )
    user_question = st.text_area("ì¶”ê°€ ì§ˆë¬¸/ìš”ì²­(ì„ íƒ, ë¹„ìš°ë©´ ê¸°ë³¸ ì§ˆë¬¸)", height=80,
                                 placeholder="ì˜ˆ: ì—°ê¸°ë ¥/í˜¸ë¶ˆí˜¸ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•´ì¤˜", key="simple_question")

    SIMPLE_TOP_N = 50
    SIMPLE_ORDER = "viewCount"
    now_kst = datetime.now(_kst_tz())

    if preset_simple == "ìµœê·¼ 12ì‹œê°„":      start_dt = now_kst - timedelta(hours=12)
    elif preset_simple == "ìµœê·¼ 24ì‹œê°„":     start_dt = now_kst - timedelta(hours=24)
    elif preset_simple == "ìµœê·¼ 48ì‹œê°„":     start_dt = now_kst - timedelta(hours=48)
    elif preset_simple == "ìµœê·¼ 1ì£¼ì¼":     start_dt = now_kst - timedelta(days=7)
    elif preset_simple == "ìµœê·¼ 1ê°œì›”":     start_dt = now_kst - timedelta(days=30)
    elif preset_simple == "ìµœê·¼ 6ê°œì›”":     start_dt = now_kst - timedelta(days=182)
    elif preset_simple == "ìµœê·¼ 1ë…„":       start_dt = now_kst - timedelta(days=365)
    elif preset_simple == "ìµœê·¼ 2ë…„":       start_dt = now_kst - timedelta(days=365*2)
    elif preset_simple == "ìµœê·¼ 3ë…„":       start_dt = now_kst - timedelta(days=365*3)
    elif preset_simple == "ìµœê·¼ 4ë…„":       start_dt = now_kst - timedelta(days=365*4)
    elif preset_simple == "ìµœê·¼ 5ë…„":       start_dt = now_kst - timedelta(days=365*5)
    elif preset_simple == "ìµœê·¼ 10ë…„":      start_dt = now_kst - timedelta(days=365*10)
    else:                                   start_dt = now_kst - timedelta(days=365)

    published_after = kst_to_rfc3339_utc(start_dt)
    published_before = kst_to_rfc3339_utc(now_kst)

    if st.button("ğŸš€ ë¶„ì„í•˜ê¸°", type="primary", key="simple_run"):
        if not YT_API_KEYS:
            st.error("YouTube API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
        elif not GEMINI_API_KEYS:
            st.error("Gemini API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
        elif not st.session_state["simple_query"].strip():
            st.warning("ë“œë¼ë§ˆ or ë°°ìš°ëª…ì„ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            # === ë™ì‹œ ì‹¤í–‰ ë½ ì‹œë„ ===
            if not lock_guard_start_or_warn():
                st.stop()
            try:
                st.session_state["s_query"] = st.session_state["simple_query"].strip()
                st.session_state["s_preset"] = preset_simple
                st.session_state["s_history"] = []
                append_log("ì‹¬í”Œ", st.session_state["s_query"], st.session_state.get("simple_question", ""))  # no-op

                status_ph = st.empty()
                with status_ph.status("ì‹¬í”Œ ëª¨ë“œ ì‹¤í–‰ ì¤‘â€¦", expanded=True) as status:
                    rt = RotatingYouTube(YT_API_KEYS, log=lambda m: status.write(m))
                    status.write(f"ğŸ” ì˜ìƒ ê²€ìƒ‰ ì¤‘â€¦ ({preset_simple}, ì •ë ¬: {SIMPLE_ORDER})")
                    ids = yt_search_videos(rt, st.session_state["s_query"], SIMPLE_TOP_N,
                                           SIMPLE_ORDER, published_after, published_before, log=status.write)

                    status.write(f"ğŸï¸ ëŒ€ìƒ ì˜ìƒ: {len(ids)} â€” ë©”íƒ€ ì¡°íšŒâ€¦")
                    stats = yt_video_statistics(rt, ids, log=status.write)
                    df_stats = pd.DataFrame(stats)
                    st.session_state["s_df_stats"] = df_stats

                    # ë³‘ë ¬ ëŒ“ê¸€ ìˆ˜ì§‘ (ëŒ€ëŒ“ê¸€ ì œì™¸)
                    status.write("ğŸ’¬ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘â€¦")
                    video_list = df_stats.to_dict('records')
                    prog = st.progress(0, text="ìˆ˜ì§‘ ì§„í–‰ ì¤‘")
                    log_ph = st.empty()
                    rows = parallel_collect_comments(
                        video_list=video_list,
                        rt_keys=YT_API_KEYS,
                        include_replies=False,
                        max_total_comments=MAX_TOTAL_COMMENTS,
                        max_per_video=MAX_COMMENTS_PER_VIDEO,
                        log_callback=log_ph.write,
                        prog_callback=prog.progress
                    )

                    if not rows:
                        status.update(label="âš ï¸ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", state="error")
                        st.session_state["s_df_comments"] = None
                        st.session_state["s_df_analysis"] = None
                        st.session_state["s_serialized_sample"] = ""
                        st.session_state["s_result_text"] = ""
                    else:
                        df_comments = pd.DataFrame(rows).applymap(clean_illegal)
                        st.session_state["s_df_comments"] = df_comments
                        df_analysis = sample_max_5000(df_comments)
                        st.session_state["s_df_analysis"] = df_analysis

                        s_text, _, _ = serialize_comments_for_llm(
                            df_analysis, max_rows=1500, max_chars_per_comment=280, max_total_chars=450_000
                        )
                        st.session_state["s_serialized_sample"] = s_text

                        # Gemini ë¶„ì„
                        status.write("ğŸ§  AI ë¶„ì„ ì¤‘â€¦")
                        system_instruction = (
                            "ë„ˆëŠ” ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ë¶„ì„í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. "
                            "ì•„ë˜ í‚¤ì›Œë“œì™€ ì§€ì •ëœ ê¸°ê°„ ë‚´ ëŒ“ê¸€ ìƒ˜í”Œì„ ë°”íƒ•ìœ¼ë¡œ, ì „ë°˜ì  ë°˜ì‘ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ë¼. "
                            "í•µì‹¬ í¬ì¸íŠ¸ë¥¼ í•­ëª©í™”í•˜ê³ , ê¸/ë¶€ì •/ì¤‘ë¦½ì˜ ëŒ€ëµì  ë¹„ìœ¨ê³¼ ëŒ€í‘œ ì½”ë©˜íŠ¸(10ê°œë¯¸ë§Œ)ë¥¼ ì˜ˆì‹œë¡œ ì œì‹œí•˜ë¼. "
                            "í‚¤ì›Œë“œê°€ ì¸ë¬¼ëª…ì´ë©´ ì¸ë¬¼ ì¤‘ì‹¬, ë“œë¼ë§ˆëª…ì´ë©´ ë“œë¼ë§ˆ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ë¼. "
                            "ë°˜ë“œì‹œ ëª¨ë“  ëŒ“ê¸€ì„ ì½ê³  ë‹µë³€í•˜ë¼."
                        )
                        default_q = f"{preset_simple} ê¸°ì¤€ìœ¼ë¡œ '{st.session_state['s_query']}'ì— ëŒ€í•œ ìœ íŠœë¸Œ ëŒ“ê¸€ ë°˜ì‘ì„ ìš”ì•½í•´ì¤˜."
                        prompt_q = (st.session_state.get("simple_question", "").strip() or default_q)
                        payload = (
                            f"[í‚¤ì›Œë“œ]: {st.session_state['s_query']}\n"
                            f"[ì§ˆë¬¸]: {prompt_q}\n"
                            f"[ê¸°ê°„]: {preset_simple}\n\n"
                            f"[ëŒ“ê¸€ ìƒ˜í”Œ]:\n{s_text}\n"
                        )
                        out = call_gemini_rotating(GEMINI_MODEL, GEMINI_API_KEYS, system_instruction, payload,
                                                   timeout_s=GEMINI_TIMEOUT, max_tokens=GEMINI_MAX_TOKENS,
                                                   on_rotate=lambda i, k: status.write(f"ğŸ” Gemini í‚¤ ì „í™˜ â†’ #{i+1}"))
                        st.session_state["s_result_text"] = out
                        st.session_state["s_history"].append((prompt_q, out))
                        status.update(label="ğŸ‰ ë¶„ì„ ì™„ë£Œ", state="complete")
                status_ph.empty()
            finally:
                release_lock()

    # ê²°ê³¼/ì¶”ê°€ì§ˆë¬¸/ì‹œê°í™”/ë‹¤ìš´ë¡œë“œ
    s_df_comments = st.session_state.get("s_df_comments")
    s_df_analysis = st.session_state.get("s_df_analysis")
    s_df_stats = st.session_state.get("s_df_stats")

    if s_df_comments is not None and not s_df_comments.empty:
        st.success(f"ìˆ˜ì§‘ ì™„ë£Œ â€” ì „ì²´ {len(s_df_comments):,}ê°œ / ìƒ˜í”Œ {len(s_df_analysis):,}ê°œ")

    if st.session_state.get("s_result_text"):
        with st.expander("ğŸ§  AI ë¶„ì„ ê²°ê³¼", expanded=True):
            st.markdown(st.session_state["s_result_text"])
        if st.session_state.get("s_history"):
            st.markdown("### ğŸ“ ì¶”ê°€ ì§ˆë¬¸ íˆìŠ¤í† ë¦¬")
            for i, (q, a) in enumerate(st.session_state["s_history"][1:], start=1):
                with st.expander(f"Q{i}. {q}", expanded=True):
                    st.markdown(a or "_ì‘ë‹µ ì—†ìŒ_")
        st.markdown("#### â• ì¶”ê°€ ì§ˆë¬¸í•˜ê¸°")
        st.text_input("ì¶”ê°€ ì§ˆë¬¸", placeholder="ì˜ˆ: ì£¼ì—°ë°°ìš°ë“¤ì— ëŒ€í•œ ë°˜ì‘ì€ ì–´ë•Œ?",
                      key="simple_follow_q", on_change=handle_followup_simple)
        st.button("ì§ˆë¬¸ ë³´ë‚´ê¸°", key="simple_follow_btn", on_click=handle_followup_simple)

    render_quant_viz(s_df_comments, s_df_stats, scope_label="(KST ê¸°ì¤€)")
    render_downloads(s_df_comments, s_df_analysis, s_df_stats, prefix="simple")

    if st.button("ğŸ’¾ ì„¸ì…˜ ì €ì¥í•˜ê¸°", key="simple_save_session"):
        name, drive_id = save_current_session(None)
        if drive_id:
            st.success(f"ì„¸ì…˜ ì €ì¥ ì™„ë£Œ Â· Drive í´ë”: https://drive.google.com/drive/folders/{drive_id}")
        else:
            st.success(f"ì„¸ì…˜ ì €ì¥ ì™„ë£Œ(ë¡œì»¬) Â· {name}")

# ===================== 2) ê³ ê¸‰ ëª¨ë“œ =====================
with tab_advanced:
    st.subheader("ê³ ê¸‰ ëª¨ë“œ â€” 4ë‹¨ê³„ë¡œ ì„¸ë°€ ì œì–´ (ì‹¬í”Œê³¼ ë™ë“± ë¡œì§/ì‹œê°í™”)")

    mode = st.radio("ëª¨ë“œ", ["ê²€ìƒ‰ ëª¨ë“œ", "URL ì§ì ‘ ì…ë ¥ ëª¨ë“œ"],
                    index=(0 if st.session_state.get("mode", "ê²€ìƒ‰ ëª¨ë“œ") == "ê²€ìƒ‰ ëª¨ë“œ" else 1),
                    horizontal=True, key="adv_mode_radio")
    if mode != st.session_state["mode"]:
        st.session_state["mode"] = mode
        st.session_state["focus_step"] = 1

    include_replies = st.checkbox("ëŒ€ëŒ“ê¸€ í¬í•¨", value=False, key="adv_include_replies")

    # â‘  ì˜ìƒëª©ë¡ì¶”ì¶œ
    expanded1 = (st.session_state["focus_step"] == 1)
    with st.expander("â‘  ì˜ìƒëª©ë¡ì¶”ì¶œ", expanded=expanded1):
        published_after = published_before = None
        if st.session_state["mode"] == "ê²€ìƒ‰ ëª¨ë“œ":
            st.markdown("**ì—…ë¡œë“œ ê¸°ê°„ (KST)**")
            preset = st.radio("í”„ë¦¬ì…‹", ["ìµœê·¼ 12ì‹œê°„", "ìµœê·¼ 30ì¼", "ìµœê·¼ 1ë…„", "ì§ì ‘ ì…ë ¥"],
                              horizontal=True, key="adv_preset")
            now_kst = datetime.now(_kst_tz())
            if preset == "ìµœê·¼ 12ì‹œê°„":
                start_dt = now_kst - timedelta(hours=12); end_dt = now_kst
            elif preset == "ìµœê·¼ 30ì¼":
                start_dt = now_kst - timedelta(days=30); end_dt = now_kst
            elif preset == "ìµœê·¼ 1ë…„":
                start_dt = now_kst - timedelta(days=365); end_dt = now_kst
            else:
                c1, c2 = st.columns(2)
                sd = c1.date_input("ì‹œì‘ì¼", now_kst.date()-timedelta(days=30), key="adv_sd")
                stime = c1.time_input("ì‹œì‘ ì‹œ:ë¶„", value=datetime.min.time().replace(hour=0, minute=0), key="adv_stime")
                ed = c2.date_input("ì¢…ë£Œì¼", now_kst.date(), key="adv_ed")
                etime = c2.time_input("ì¢…ë£Œ ì‹œ:ë¶„", value=datetime.min.time().replace(hour=23, minute=59), key="adv_etime")
                start_dt = datetime.combine(sd, stime, tzinfo=_kst_tz())
                end_dt = datetime.combine(ed, etime, tzinfo=_kst_tz())
            published_after = kst_to_rfc3339_utc(start_dt)
            published_before = kst_to_rfc3339_utc(end_dt)

            c1, c2, c3 = st.columns([3, 1, 1])
            keyword = c1.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", st.session_state.get("last_keyword", "") or "", key="adv_keyword")
            top_n = c2.number_input("TOP N", min_value=1, value=50, step=1, key="adv_topn")
            order = c3.selectbox("ì •ë ¬", ["relevance", "viewCount"], key="adv_order")
        else:
            keyword = None; top_n = None; order = None
            urls_main = st.text_area("URL/ID ëª©ë¡ (ì¤„ë°”ê¿ˆ êµ¬ë¶„)", height=160,
                                     placeholder="https://youtu.be/XXXXXXXXXXX\nXXXXXXXXXXX\n...", key="adv_urls")

        if st.button("ëª©ë¡ ê°€ì ¸ì˜¤ê¸°", use_container_width=True, key="adv_fetch_list"):
            if not YT_API_KEYS:
                st.error("YouTube API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                rt = RotatingYouTube(YT_API_KEYS, log=lambda m: st.write(m))
                log_box = st.empty()
                def log(msg): log_box.write(msg)
                if st.session_state["mode"] == "ê²€ìƒ‰ ëª¨ë“œ":
                    st.session_state["last_keyword"] = keyword or ""
                    log("ğŸ” ê²€ìƒ‰ ì‹¤í–‰ ì¤‘â€¦")
                    ids = yt_search_videos(rt, keyword, int(top_n), order, published_after, published_before, log)
                else:
                    ids = extract_video_ids_from_text(urls_main or "")
                    if not ids:
                        st.warning("URL/IDê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."); st.stop()
                log(f"ğŸï¸ ëŒ€ìƒ ì˜ìƒ ìˆ˜: {len(ids)} â€” ë©”íƒ€/í†µê³„ ì¡°íšŒâ€¦")
                stats = yt_video_statistics(rt, ids, log)
                df = pd.DataFrame(stats)
                if not df.empty and "publishedAt" in df.columns:
                    df["publishedAt"] = (
                        pd.to_datetime(df["publishedAt"], errors="coerce", utc=True)
                        .dt.tz_convert("Asia/Seoul").dt.strftime("%Y-%m-%d %H:%M:%S")
                    )
                if "viewCount" in df.columns:
                    df = df.sort_values(by="viewCount", ascending=False).reset_index(drop=True)
                st.session_state["df_stats"] = df
                st.session_state["selected_ids"] = df["video_id"].tolist() if not df.empty else []
                st.session_state["focus_step"] = 2
                st.success(f"ëª©ë¡ ì¤€ë¹„ ì™„ë£Œ â€” ì´ {len(df)}ê°œ")
                st.rerun()

    # â‘¡ ì˜ìƒì„ íƒ ë° URLì¶”ê°€
    expanded2 = (st.session_state["focus_step"] == 2)
    with st.expander("â‘¡ ì˜ìƒì„ íƒ ë° URLì¶”ê°€", expanded=expanded2):
        df_stats = st.session_state["df_stats"]
        if df_stats is None or df_stats.empty:
            st.info("â‘ ì—ì„œ ë¨¼ì € ëª©ë¡ì„ ê°€ì ¸ì˜¤ì„¸ìš”.")
        else:
            df_show = df_stats.copy()
            df_show["select"] = df_show["video_id"].apply(lambda v: v in st.session_state["selected_ids"])
            df_view = df_show[["video_id","select","title","channelTitle","shortType","viewCount","commentCount","publishedAt","video_url"]].set_index("video_id")
            edited = st.data_editor(
                df_view,
                column_config={
                    "select": st.column_config.CheckboxColumn("ì„ íƒ", default=True),
                    "title": st.column_config.TextColumn("ì œëª©"),
                    "channelTitle": st.column_config.TextColumn("ì±„ë„"),
                    "shortType": st.column_config.TextColumn("íƒ€ì…"),
                    "viewCount": st.column_config.NumberColumn("ì¡°íšŒìˆ˜", format="%,d"),
                    "commentCount": st.column_config.NumberColumn("ëŒ“ê¸€ìˆ˜", format="%,d"),
                    "publishedAt": st.column_config.TextColumn("ì—…ë¡œë“œ(KST)"),
                    "video_url": st.column_config.LinkColumn("ìœ íŠœë¸Œ ë§í¬", display_text="ë³´ê¸°"),
                },
                use_container_width=True, num_rows="fixed", hide_index=False, key="adv_editor_table",
            )
            st.session_state["selected_ids"] = [vid for vid, row in edited.iterrows() if bool(row.get("select", False))]
            st.caption(f"ì„ íƒëœ ì˜ìƒ: {len(st.session_state['selected_ids'])} / {len(edited)}")
            csel1, csel2, _ = st.columns([1,1,6])
            if csel1.button("ì „ì²´ì„ íƒ", key="adv_select_all"):
                st.session_state["selected_ids"] = df_stats["video_id"].tolist(); st.rerun()
            if csel2.button("ì „ì²´í•´ì œ", key="adv_clear_all"):
                st.session_state["selected_ids"] = []; st.rerun()
            st.markdown("---")
            if st.session_state["mode"] == "ê²€ìƒ‰ ëª¨ë“œ":
                st.subheader("â• ì¶”ê°€ URL/ID ë³‘í•©")
                add_text = st.text_area("ì¶”ê°€í•  URL/ID (ì¤„ë°”ê¿ˆ êµ¬ë¶„)", height=100,
                                        placeholder="https://youtu.be/XXXXXXXXXXX\nXXXXXXXXXXX\n...", key="adv_add_text")
                if st.button("ì¶”ê°€ ë³‘í•© ì‹¤í–‰", key="adv_merge_btn"):
                    add_ids = extract_video_ids_from_text(add_text or "")
                    already = set(df_stats["video_id"].tolist())
                    dup = [v for v in add_ids if v in already]
                    add_ids = [v for v in add_ids if v not in already]
                    if dup: st.info(f"âš ï¸ ê¸°ì¡´ ëª©ë¡ê³¼ ì¤‘ë³µ {len(dup)}ê°œ ì œì™¸")
                    if add_ids:
                        rt = RotatingYouTube(YT_API_KEYS, log=lambda m: st.write(m))
                        add_stats = yt_video_statistics(rt, add_ids)
                        add_df = pd.DataFrame(add_stats)
                        if not add_df.empty and "publishedAt" in add_df.columns:
                            add_df["publishedAt"] = (
                                pd.to_datetime(add_df["publishedAt"], errors="coerce", utc=True)
                                .dt.tz_convert("Asia/Seoul").dt.strftime("%Y-%m-%d %H:%M:%S")
                            )
                        st.session_state["df_stats"] = (
                            pd.concat([st.session_state["df_stats"], add_df], ignore_index=True)
                            .drop_duplicates(subset=["video_id"])
                            .sort_values(by="viewCount", ascending=False)
                            .reset_index(drop=True)
                        )
                        st.session_state["selected_ids"] = list(dict.fromkeys(st.session_state["selected_ids"] + add_ids))
                        st.success(f"ì¶”ê°€ {len(add_ids)}ê°œ ë³‘í•© ì™„ë£Œ"); st.rerun()
                    else:
                        st.info("ì¶”ê°€í•  ì‹ ê·œ URL/IDê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.markdown("---")
            if st.button("ë‹¤ìŒ: ëŒ“ê¸€ì¶”ì¶œë¡œ ì´ë™", type="primary", key="adv_next_to_comments"):
                if not st.session_state["selected_ids"]:
                    st.warning("ì„ íƒëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.session_state["focus_step"] = 3
                    st.rerun()

    # â‘¢ ëŒ“ê¸€ì¶”ì¶œ
    expanded3 = (st.session_state["focus_step"] == 3)
    with st.expander("â‘¢ ëŒ“ê¸€ì¶”ì¶œ", expanded=expanded3):
        if not st.session_state["selected_ids"]:
            st.info("â‘¡ì—ì„œ ë¨¼ì € ì˜ìƒì„ ì„ íƒí•˜ì„¸ìš”.")
        else:
            st.write(f"ëŒ€ìƒ ì˜ìƒ ìˆ˜: **{len(st.session_state['selected_ids'])}**")
            include_replies_local = st.checkbox("ëŒ€ëŒ“ê¸€ í¬í•¨(ì´ ë‹¨ê³„ì—ì„œë§Œ ì ìš©)",
                                                value=include_replies, key="adv_include_replies_collect")
            if st.button("ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘", type="primary", use_container_width=True, key="adv_collect_btn"):
                if not YT_API_KEYS:
                    st.error("YouTube API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    if not lock_guard_start_or_warn():
                        st.stop()
                    try:
                        df_stats = st.session_state["df_stats"]
                        target_ids = st.session_state["selected_ids"]
                        if df_stats is not None and not df_stats.empty and "viewCount" in df_stats.columns:
                            video_list = df_stats[df_stats["video_id"].isin(target_ids)].sort_values("viewCount", ascending=False).to_dict('records')
                        else:
                            video_list = [{"video_id": vid, "title": "", "shortType": "Clip"} for vid in target_ids]
                        prog = st.progress(0, text="ìˆ˜ì§‘ ì§„í–‰")
                        log_ph = st.empty()
                        rows = parallel_collect_comments(
                            video_list=video_list,
                            rt_keys=YT_API_KEYS,
                            include_replies=st.session_state.get("adv_include_replies_collect", False),
                            max_total_comments=MAX_TOTAL_COMMENTS,
                            max_per_video=MAX_COMMENTS_PER_VIDEO,
                            log_callback=log_ph.write,
                            prog_callback=prog.progress
                        )
                        if rows:
                            df_comments = pd.DataFrame(rows).applymap(clean_illegal)
                            st.session_state["df_comments"] = df_comments
                            df_analysis = sample_max_5000(df_comments)
                            st.session_state["df_analysis"] = df_analysis
                            a_text, _, _ = serialize_comments_for_llm(
                                df_analysis, max_rows=1500, max_chars_per_comment=280, max_total_chars=450_000
                            )
                            st.session_state["adv_serialized_sample"] = a_text
                            st.success("ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ! í•„ìš” ì‹œ ì•„ë˜ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í›„ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™í•˜ì„¸ìš”.")
                        else:
                            st.warning("ëŒ“ê¸€ì´ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    finally:
                        release_lock()
                if st.button("ë‹¤ìŒ: AIë¶„ì„ìœ¼ë¡œ ì´ë™", type="primary", key="adv_go_to_step4"):
                    st.session_state["focus_step"] = 4
                    st.rerun()

    # â‘£ AIë¶„ì„
    expanded4 = (st.session_state["focus_step"] == 4)
    with st.expander("â‘£ AIë¶„ì„", expanded=expanded4):
        df_analysis = st.session_state["df_analysis"]
        if df_analysis is None or df_analysis.empty:
            st.info("â‘¢ì—ì„œ ëŒ“ê¸€ ìˆ˜ì§‘ì„ ì™„ë£Œí•˜ë©´ ì—¬ê¸°ì— ë¶„ì„ ê¸°ëŠ¥ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")
        else:
            st.write(f"ë¶„ì„ ëŒ€ìƒ ìƒ˜í”Œ ëŒ“ê¸€ ìˆ˜: **{len(df_analysis):,}** (ìµœëŒ€ 5,000ê°œ)")
            analysis_keyword = st.text_input("ê´€ë ¨ í‚¤ì›Œë“œ(ë¶„ì„ ì»¨í…ìŠ¤íŠ¸)",
                                             value=st.session_state.get("last_keyword", ""),
                                             placeholder="ì˜ˆ: ìœ¤ë‘ì¤€", key="adv_analysis_keyword")
            user_question_adv = st.text_area("ì‚¬ìš©ì ì§ˆë¬¸", height=80, placeholder="ì˜ˆ: ìµœê·¼ ë°˜ì‘ì€?", key="adv_user_question")

            if st.button("âœ¨ AI ë¶„ì„ ì‹¤í–‰", type="primary", key="adv_run_gem"):
                if not GEMINI_API_KEYS:
                    st.error("Gemini API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    if not lock_guard_start_or_warn():
                        st.stop()
                    try:
                        append_log("ê³ ê¸‰", analysis_keyword, user_question_adv)  # no-op
                        st.session_state["adv_history"] = []
                        st.session_state["adv_followups"] = []
                        a_text = st.session_state.get("adv_serialized_sample", "") or serialize_comments_for_llm(df_analysis)[0]
                        system_instruction = (
                            "ë„ˆëŠ” ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ë¶„ì„í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. "
                            "ì•„ë˜ í‚¤ì›Œë“œì™€ ëŒ“ê¸€ ìƒ˜í”Œì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ë¼. "
                            "í•µì‹¬ í¬ì¸íŠ¸ë¥¼ í•­ëª©í™”í•˜ê³ , ëŒ€ëµì  ë¹„ìœ¨ê³¼ ëŒ€í‘œ ì½”ë©˜íŠ¸(10ê°œë¯¸ë§Œ)ë„ ì œì‹œí•˜ë¼. ì¶œë ¥ì€ í•œêµ­ì–´. "
                            "í‚¤ì›Œë“œê°€ ë°°ìš°ëª…(ì‚¬ëŒì´ë¦„)ì´ë©´ ë°°ìš° ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ë¼. "
                            "ë°˜ë“œì‹œ ëª¨ë“  ëŒ“ê¸€ì„ ì½ê³  ë‹µë³€í•˜ë¼."
                        )
                        user_payload = f"[í‚¤ì›Œë“œ]: {analysis_keyword}\n[ì§ˆë¬¸]: {user_question_adv}\n\n[ëŒ“ê¸€ ìƒ˜í”Œ]:\n{a_text}\n"
                        out = call_gemini_rotating(GEMINI_MODEL, GEMINI_API_KEYS, system_instruction, user_payload)
                        st.session_state["adv_result_text"] = out
                        st.session_state["adv_history"].append((user_question_adv or "ìµœê·¼ ë°˜ì‘ ìš”ì•½", out))
                        st.success("AI ë¶„ì„ ì™„ë£Œ")
                    finally:
                        release_lock()

            if st.session_state.get("adv_result_text"):
                st.markdown("#### ğŸ“„ ë¶„ì„ ê²°ê³¼")
                st.markdown(st.session_state["adv_result_text"])
                if st.session_state["adv_followups"]:
                    st.markdown("### ğŸ“ ì¶”ê°€ ì§ˆë¬¸ íˆìŠ¤í† ë¦¬")
                    for i, (q, a) in enumerate(st.session_state["adv_followups"], start=1):
                        with st.expander(f"Q{i}. {q}", expanded=True):
                            st.markdown(a or "_ì‘ë‹µ ì—†ìŒ_")
                st.markdown("#### â• ì¶”ê°€ ì§ˆë¬¸í•˜ê¸°")
                st.text_input("ì¶”ê°€ ì§ˆë¬¸", placeholder="ì˜ˆ: ê¸/ë¶€ì • í‚¤ì›Œë“œ Top5ëŠ”?",
                              key="adv_follow_q", on_change=handle_followup_advanced)
                st.button("ì§ˆë¬¸ ë³´ë‚´ê¸°(ê³ ê¸‰)", key="adv_follow_btn", on_click=handle_followup_advanced)

                if st.button("ğŸ’¾ ì„¸ì…˜ ì €ì¥í•˜ê¸°", key="adv_save_session_analysis"):
                    name, drive_id = save_current_session(None)
                    if drive_id:
                        st.success(f"ì„¸ì…˜ ì €ì¥ ì™„ë£Œ Â· Drive í´ë”: https://drive.google.com/drive/folders/{drive_id}")
                    else:
                        st.success(f"ì„¸ì…˜ ì €ì¥ ì™„ë£Œ(ë¡œì»¬) Â· {name}")

    render_quant_viz(st.session_state.get("df_comments"), st.session_state.get("df_stats"), scope_label="(KST ê¸°ì¤€)")
    render_downloads(st.session_state.get("df_comments"), st.session_state.get("df_analysis"),
                     st.session_state.get("df_stats"), prefix=f"adv_{len(st.session_state.get('selected_ids', []))}vids")

    if st.button("ğŸ’¾ ì„¸ì…˜ ì €ì¥í•˜ê¸°", key="adv_save_session_comments"):
        name, drive_id = save_current_session(None)
        if drive_id:
            st.success(f"ì„¸ì…˜ ì €ì¥ ì™„ë£Œ Â· Drive í´ë”: https://drive.google.com/drive/folders/{drive_id}")
        else:
            st.success(f"ì„¸ì…˜ ì €ì¥ ì™„ë£Œ(ë¡œì»¬) Â· {name}")

# ===================== 3) ì„¸ì…˜ ì•„ì¹´ì´ë¸Œ =====================
with tab_sessions:
    st.subheader("ì €ì¥ëœ ì„¸ì…˜ ì•„ì¹´ì´ë¸Œ")

    drive_ok = bool(GDRIVE_PARENT_FOLDER_ID and GDRIVE_KEYS)
    if not drive_ok:
        st.info("Drive ì„¤ì •ì´ ì—†ì–´ ë¡œì»¬ ì„¸ì…˜ë§Œ í‘œì‹œë©ë‹ˆë‹¤.")
        sess_list = list_sessions_local()
        if not sess_list:
            st.info("ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            selected = st.selectbox("ì„¸ì…˜ ì„ íƒ(ë¡œì»¬)", sess_list, key="sess_select_local")
            sess_path = os.path.join(SESS_DIR, selected)
            qa_file = os.path.join(sess_path, "qa.json")
            if os.path.exists(qa_file):
                with open(qa_file, encoding="utf-8") as f:
                    qa_data = json.load(f)
                st.write("### ì§ˆë¬¸/ì‘ë‹µ")
                for i,(q,a) in enumerate(qa_data.get("simple_history",[]),1):
                    with st.expander(f"[ì‹¬í”Œ Q{i}] {q}", expanded=False):
                        st.markdown(a)
                for i,(q,a) in enumerate(qa_data.get("adv_history",[]),1):
                    with st.expander(f"[ê³ ê¸‰ Q{i}] {q}", expanded=False):
                        st.markdown(a)
            if st.button("ğŸ“¦ ZIP ë§Œë“¤ê¸°/ìƒˆë¡œê³ ì¹¨", key="sess_zip_build_local"):
                zip_session(selected); st.success("ZIP ìƒì„±/ê°±ì‹  ì™„ë£Œ")
            zip_path = os.path.join(SESS_DIR, f"{selected}.zip")
            if os.path.exists(zip_path):
                with open(zip_path, "rb") as f:
                    st.download_button("â¬‡ï¸ ì„¸ì…˜ ì „ì²´ ë‹¤ìš´ë¡œë“œ (ZIP)", data=f.read(), file_name=f"{selected}.zip")
            st.write("### ì„¸ì…˜ í´ë” íŒŒì¼ (CSV/JSON/PNG)")
            for fn in sorted(os.listdir(sess_path)):
                p = os.path.join(sess_path, fn)
                if os.path.isfile(p):
                    with open(p, "rb") as f:
                        st.download_button(f"â¬‡ï¸ {fn}", data=f.read(), file_name=fn, key=f"dl_{selected}_{fn}")
    else:
        # Drive ì„¸ì…˜ ì „ì²´ ëª©ë¡ í‘œì‹œ
        try:
            rd = RotatingDrive(GDRIVE_KEYS, log=lambda m: st.write(m))
            folders = drive_list_folders(rd, GDRIVE_PARENT_FOLDER_ID)
            if not folders:
                st.info("Driveì— ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                options = [f["name"] for f in folders]
                idx_map = {f["name"]: f["id"] for f in folders}
                selected_name = st.selectbox("ì„¸ì…˜ ì„ íƒ(Drive)", options, key="sess_select_drive")
                selected_id = idx_map.get(selected_name)
                if selected_id:
                    files = drive_list_files_in_folder(rd, selected_id)
                    manifest = next((x for x in files if x["name"] == "manifest.json"), None)
                    if manifest:
                        st.markdown(f"- **Drive í´ë” ë§í¬:** https://drive.google.com/drive/folders/{selected_id}")
                    st.write("### íŒŒì¼ ëª©ë¡ (Drive)")
                    for fobj in files:
                        name = fobj.get("name")
                        size = fobj.get("size", "0")
                        vlink = fobj.get("webViewLink")
                        dlink = fobj.get("webContentLink")
                        mt = fobj.get("mimeType","")
                        created = fobj.get("createdTime","")
                        col1, col2 = st.columns([6,4])
                        with col1:
                            st.markdown(f"**{name}**  \níƒ€ì…: `{mt}` Â· ìƒì„±: `{created}` Â· í¬ê¸°: {size}")
                        with col2:
                            if vlink:
                                st.link_button("ì—´ê¸°", vlink, help="ë¸Œë¼ìš°ì €ì—ì„œ ë³´ê¸°", key=f"view_{selected_id}_{name}")
                            if dlink:
                                st.link_button("ë‹¤ìš´ë¡œë“œ", dlink, help="ë°”ë¡œ ë‹¤ìš´ë¡œë“œ", key=f"dl_{selected_id}_{name}")
                    st.caption("â€» Drive ë§í¬ëŠ” ì ‘ê·¼ ê¶Œí•œì´ ìˆëŠ” ì‚¬ìš©ìë§Œ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.warning(f"Drive ì•„ì¹´ì´ë¸Œ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ===================== ì´ˆê¸°í™” ë²„íŠ¼ =====================
st.markdown("---")
if st.button("ğŸ”„ ì´ˆê¸°í™” í•˜ê¸°", type="secondary"):
    for k in list(st.session_state.keys()):
        del st.session_state[k]
    st.rerun()
